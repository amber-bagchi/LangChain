from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from huggingface_hub import login
from langchain.schema.runnable import RunnableParallel, RunnableBranch, RunnableLambda
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import Literal
import os
import sys

# 1Ô∏è‚É£ Load variables from .env if present
load_dotenv()

# 2Ô∏è‚É£ Get the token from env variable
token = os.getenv("HUGGINGFACEHUB_ACCESS_TOKEN")

# 3Ô∏è‚É£ Check if token is loaded
if not token:
    print("‚ùå ERROR: Hugging Face API token not found.")
    print("Make sure you have either:")
    print(" - A .env file with: HUGGINGFACEHUB_ACCESS_TOKEN=your_token_here")
    print(" - Or set the token in your system environment variables")
    sys.exit(1)
else:
    print("‚úÖ Hugging Face token loaded successfully.")
    
# 4Ô∏è‚É£ Log in to Hugging Face (stores token locally)
try:
    login(token=token)
    print("üîë Logged in to Hugging Face successfully.")
except Exception as e:
    print("‚ö†Ô∏è Warning: Could not log in via huggingface_hub:", e)
    
# 5Ô∏è‚É£ Create HuggingFaceEndpoint
llm1 = HuggingFaceEndpoint(
    
    repo_id="meta-llama/Llama-3.3-70B-Instruct",  # Model
    task="text-generation",
    huggingfacehub_api_token=token   
)

# 6Ô∏è‚É£ Create Model
model1 = ChatHuggingFace(llm=llm1)

# 7Ô∏è‚É£ Create Parser
parser = StrOutputParser()

class Feedback(BaseModel):
    sentiment: Literal['positive', 'negative'] = Field(description='Give the sentiment of the following')

parser2 = PydanticOutputParser(pydantic_object=Feedback)

# 8Ô∏è‚É£ Create Prompt
prompt1 = PromptTemplate(
    template= 'Classify the sentiment of the following feedback text into positive or negative \n {feedback} \n {format_instruction}',
    input_variables=['feedback'],
    partial_variables={'format_instruction': parser2.get_format_instructions()}
)

classify_chain = prompt1 | model1 | parser2

# result1  = classify_chain.invoke({'feedback': 'This is a terrible smartphone. I do not recommend it.'}).sentiment

# print(result1)

prompt2 = PromptTemplate(
    template = 'Write an appropriate response to this positive feedback \n {feedback}',
    input_variables=['feedback']
)

prompt3 = PromptTemplate(
    template = 'Write an appropriate response to this negative feedback \n {feedback}',
    input_variables=['feedback']
)

branch_chain = RunnableBranch(
    (lambda x:x.sentiment == 'positive', prompt2|model1|parser),
    (lambda x:x.sentiment == 'negative', prompt3|model1|parser),
    RunnableLambda(lambda x: "Could not find sentiment")
)

final_chain = classify_chain | branch_chain

result = final_chain.invoke({'feedback': 'This is a terrible smartphone. I do not recommend it.'})

print(result)

final_chain.get_graph().print_ascii()